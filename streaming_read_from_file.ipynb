{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "streaming_read_from_file.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marciogsantana/Ciencia-de-dados/blob/main/streaming_read_from_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ERkmnmHys0"
      },
      "source": [
        "**Exemplo de Streaming Para Leitura de Arquivos de Um Diretório**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soGy-kVNHys4"
      },
      "source": [
        "#cria a seção a ser utiliza para estabelecer a conexão \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredNetworkWordCount\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnFlHQRvHytB"
      },
      "source": [
        "#cria o dataframe que será responsável por ler cada uma das linhas recebidas dos arquivos adicionados no diretório\n",
        "files_dir = spark.readStream\\\n",
        "    .format(\"text\")\\\n",
        "    .option(\"inferSchema\", \"true\")\\\n",
        "    .option(\"maxFilesPerTrigger\", 1)\\\n",
        "    .load(\"/home/tulio/Documents/Aulas_PFC/arquivos/*.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1EM9KVRHytC",
        "outputId": "47d76d63-22ec-4488-929b-72545dcadaec"
      },
      "source": [
        "#verifica se criou o streaming\n",
        "files_dir.isStreaming"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjkrDYQmHytG"
      },
      "source": [
        "# Divide as linhas recebidas em cada palavra\n",
        "words = files_dir.select(\n",
        "   explode(\n",
        "       split(files_dir.value, \" \")\n",
        "   ).alias(\"word\")\n",
        ")\n",
        "\n",
        "# cria o novo dataframe a ser responsável por contar a quantidade de palavras digitadas\n",
        "wordCounts = words.groupBy(\"word\").count()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JrHaoWtHytH"
      },
      "source": [
        "#ordena as palavras que mais aparecem\n",
        "from pyspark.sql.functions import desc\n",
        "wordCounts = wordCounts.sort(desc(\"count\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnjGnQWwHytI"
      },
      "source": [
        "# Define a consulta (query) e como deve ser realizada a saída (sink) para o stream criado \n",
        "query = wordCounts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start() #inicia a \"query\"\n",
        "\n",
        "\n",
        "query.awaitTermination() #aguarda até que a \"streaming query\" termine "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPE22T5nHytJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}