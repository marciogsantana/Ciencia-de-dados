{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "streaming_read_from_file_over_Event_Time.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marciogsantana/Ciencia-de-dados/blob/main/streaming_read_from_file_over_Event_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQptuzjJLLY"
      },
      "source": [
        "**Streaming de Dados em Arquivos Sobre Event-Time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JAY75HXJLLc"
      },
      "source": [
        "#cria a seção a ser utiliza para estabelecer a conexão \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "from pyspark.sql.functions import window\n",
        "from pyspark.sql.functions import desc, asc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredCountFile\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZas4za6JLLd"
      },
      "source": [
        "#definindo um esquema para os dados\n",
        "from pyspark.sql.types import StructType\n",
        "\n",
        "\n",
        "userSchema = StructType().add(\"timestamp\", \"timestamp\").add(\"word\", \"string\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTK0f8c2JLLe"
      },
      "source": [
        "#cria o dataframe que será responsável por ler cada uma das linhas recebidas dos arquivos adicionados no diretório\n",
        "files_dir = spark.readStream\\\n",
        "    .format(\"csv\")\\\n",
        "    .schema(userSchema)\\\n",
        "    .option('includeTimestamp', 'true')\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .option(\"delimiter\", \";\")\\\n",
        "    .option(\"maxFilesPerTrigger\", 1)\\\n",
        "    .load(\"/home/tulio/Documents/Aulas_PFC/arquivos_csv/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_6vRnVtJLLf",
        "outputId": "38cc527b-9a25-4e59-e2a2-f8d578648589"
      },
      "source": [
        "#print do esquema\n",
        "files_dir.printSchema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.printSchema of DataFrame[timestamp: timestamp, word: string]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVV_G21xJLLg"
      },
      "source": [
        "# Divide as linhas recebidas em cada palavra\n",
        "words = files_dir.select(\n",
        "   explode(\n",
        "       split(files_dir.word, \" \")\n",
        "   ).alias(\"word\"), files_dir.timestamp\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaDOKRVxJLLh",
        "outputId": "b78dfd1f-6974-420e-a97c-d6c0849c5b83"
      },
      "source": [
        "words.isStreaming"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz7WXFLfJLLh"
      },
      "source": [
        "#agrupa os dados através da janela de tempo e computa sobre cada um dos grupos\n",
        "windowedCounts = words.groupBy(\n",
        "    window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n",
        "    words.word\n",
        ").count()#.sort(asc(\"window\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ-h02NxJLLi"
      },
      "source": [
        "# Define a consulta (query) e como deve ser realizada a saída (sink) para o stream criado \n",
        "query = windowedCounts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .option('truncate', 'false')\\\n",
        "    .start()\n",
        "\n",
        "\n",
        "query.awaitTermination() #aguarda até que a \"streaming query\" termine "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m8Mjpf4JLLi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}